{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSPKw9FKZ5jTQsKVaPi5ey"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Día 3 – De cero a Kaggle: construyendo el proyecto completo\n","\n","## 1. Contexto del Día 3\n","\n","Si el Día 1 fue organizarme y el Día 2 fue entender qué hacen otros,\n","el Día 3 ha sido el día de trabajar de verdad.\n","\n","Ha sido el día más largo con diferencia. No en horas seguidas,\n","sino en intensidad y en cantidad de decisiones tomadas.\n","He pasado de tener carpetas vacías a tener un notebook completo,\n","un score real en Kaggle y este diario para contarlo.\n","\n","No ha sido un camino recto. Ha habido errores, warnings,\n","un modelo que no funcionaba y momentos en los que no entendía\n","qué estaba pasando. Pero cada problema tenía solución,\n","y eso también es parte del aprendizaje.\n","\n","## 2. Herramientas y entorno de trabajo\n","\n","Empecé el proyecto en **VS Code** con los datos en local.\n","A mitad del proceso migré a **Google Colab** porque mi ordenador\n","es antiguo y el entrenamiento de LightGBM empezaba a ser pesado.\n","\n","El cambio fue más sencillo de lo que pensaba: solo tuve que\n","adaptar las rutas de los ficheros y montar Google Drive.\n","El resto del notebook funcionó exactamente igual.\n","\n","Aprendizaje: separar las rutas en variables desde el principio\n","(sin rutas hardcodeadas) hace que migrar de entorno sea trivial.\n","Eso no lo hubiera pensado solo al principio, pero ahora lo\n","haré así en todos mis proyectos.\n","\n","## 3. Estructura del proyecto\n","\n","Antes de escribir una sola línea de código, definí la estructura\n","completa del notebook siguiendo la ML Checklist:\n","\n","| Etapa | Sección |\n","|---|---|\n","| Business Understanding | §1 ML Canvas |\n","| Data Understanding | §3 §4 §5 EDA |\n","| Data Preparation | §6 §7 Feature Engineering y Preprocesado |\n","| Modeling | §8 Modelos |\n","| Evaluation | §9 Evaluación |\n","| Deployment | §10 Submission |\n","\n","Tener este mapa antes de empezar me ayudó a no perderme\n","y a saber en todo momento en qué etapa estaba.\n","\n","## 4. Lo que he construido hoy\n","\n","### 4.1 ML Canvas\n","Antes de tocar los datos escribí el ML Canvas completo:\n","problema de negocio, stakeholders, tipo de ML, variable objetivo,\n","métricas y riesgos. Parece burocrático pero tiene mucho valor:\n","me obligó a pensar en el problema como un Data Scientist,\n","no solo como alguien que entrena modelos.\n","\n","El ejercicio más útil fue definir los **trade-offs del modelo**:\n","un falso negativo (no detectar un cliente que transaccionará)\n","tiene un coste distinto a un falso positivo (alertar sobre\n","un cliente que no lo hará). Esa reflexión luego influyó\n","en cómo interpreté los resultados.\n","\n","### 4.2 EDA\n","El dataset tiene 200.000 clientes y 200 variables numéricas\n","completamente anonimizadas. Sin nombres, sin unidades, sin contexto.\n","\n","Lo más interesante del EDA fue el **hallazgo de las filas sintéticas**\n","en el test. Kaggle rellenó el conjunto de test con filas duplicadas\n","para igualar su tamaño al de train. Esto se ve claramente en el\n","gráfico de frecuencias: los valores pares dominan sobre los impares,\n","una firma matemática perfecta de la duplicación.\n","\n","Este descubrimiento no lo hice yo solo, está documentado por\n","los mejores participantes de la competición. Pero entenderlo,\n","reproducirlo y explicarlo ya es un mérito en sí mismo.\n","\n","### 4.3 Feature Engineering\n","Aquí es donde se gana o se pierde esta competición.\n","\n","Construí **200 variables de frecuencia** adicionales: para cada\n","variable y cada cliente, conté cuántas veces aparece ese valor\n","exacto en el test real (sin sintéticos). Esa frecuencia codifica\n","la \"rareza\" de cada valor, una señal con alto poder predictivo.\n","\n","El dataset pasó de 200 a 400 features.\n","\n","Un detalle técnico importante: la primera versión del código\n","generaba un `PerformanceWarning` de pandas por fragmentación\n","del DataFrame. Lo resolví usando `pd.concat` en lugar de\n","asignar columnas una a una en el bucle. Pequeño detalle,\n","gran diferencia en rendimiento y en calidad del código.\n","\n","### 4.4 Modelado\n","Entrené dos modelos:\n","\n","**Baseline — Regresión Logística:** ROC-AUC 0.8615\n","**Modelo principal — LightGBM:** ROC-AUC 0.8887\n","\n","Aquí tuve el problema más frustrante del día. El LightGBM\n","inicial daba un ROC-AUC de 0.66, por debajo del baseline.\n","Eso nunca debería pasar y es una señal clara de que algo\n","está mal, no en los datos sino en la configuración.\n","\n","El diagnóstico reveló el problema: las probabilidades predichas\n","estaban todas en un rango ridículamente estrecho (0.106 a 0.137),\n","el modelo no discriminaba nada. La causa era el parámetro\n","`is_unbalance=True` combinado con el early stopping,\n","que estaba colapsando el modelo.\n","\n","La solución fue doble: sustituir `is_unbalance` por\n","`scale_pos_weight` (calculado dinámicamente como el ratio\n","entre clase negativa y positiva, ~8.95 en este dataset)\n","y eliminar el early stopping que interfería con el entrenamiento.\n","\n","Resultado: ROC-AUC 0.8887, rango de probabilidades 0.006 a 0.948.\n","\n","### 4.5 Evaluación\n","Analicé el impacto del umbral de clasificación.\n","El umbral por defecto de 0.5 casi nunca es el óptimo\n","con desbalance severo.\n","\n","Con **umbral 0.50:** recall 74%, precision 38%.\n","Con **umbral 0.66 (óptimo F1):** recall 54%, precision 55%.\n","\n","Ninguno es mejor en absoluto. Depende del negocio:\n","si perder un cliente que transacciona es muy caro, usas 0.50.\n","Si los recursos comerciales son limitados, usas 0.66.\n","Esa decisión no la toma el modelo, la toma el negocio.\n","\n","### 4.6 Submission a Kaggle\n","```\n","Public Score  : 0.88874\n","Private Score : 0.88667\n","```\n","\n","Score competitivo dentro del rango de los participantes\n","de la competición original (ganador: ~0.926).\n","La competición cerró hace 7 años así que es una late submission,\n","pero el score es real y verificable en mi perfil de Kaggle.\n","\n","## 5. Errores y problemas del día\n","\n","**Error 1 — PerformanceWarning en pandas:**\n","Asignación de columnas en bucle. Solución: pd.concat.\n","\n","**Error 2 — LightGBM con ROC-AUC 0.66:**\n","is_unbalance + early stopping colapsando el modelo.\n","Solución: scale_pos_weight + n_estimators fijo.\n","\n","**Error 3 — Typo en título de gráfico:**\n","\"qeu\" en lugar de \"que\". Sin impacto técnico pero\n","importante para un portfolio profesional.\n","\n","Estos errores no son fracasos. Son exactamente el tipo\n","de problemas que aparecen en proyectos reales y saber\n","diagnosticarlos y resolverlos es una habilidad en sí misma.\n","\n","## 6. Aprendizajes clave del Día 3\n","\n","**Sobre el proceso:**\n","Un notebook de portfolio no es solo código que funciona.\n","Es código que funciona, bien documentado, con las decisiones\n","justificadas y los errores resueltos y explicados.\n","\n","**Sobre LightGBM:**\n","Los modelos basados en árboles no requieren estandarización.\n","El parámetro scale_pos_weight es más estable que is_unbalance\n","para manejar desbalance de clases en este dataset.\n","El early stopping puede interferir con el entrenamiento\n","dependiendo del entorno de ejecución.\n","\n","**Sobre el feature engineering:**\n","El frequency encoding sobre las filas reales del test\n","fue la técnica con mayor impacto en el score final.\n","Más que los hiperparámetros del modelo, más que\n","cualquier otra decisión técnica del proyecto.\n","\n","**Sobre mí:**\n","Escribir a mano cada línea del notebook, en lugar de\n","copiar y pegar, ponerme a buscar en los libros,\n","en otros trabajos los codigos, preguntar como puedo hacer\n","lo que tengo en mi cabeza es una gran aprendizaje. Ahora tengo mi\n","cuaderno de apunte de codigos lleno, porque para ser honestos\n","hacer esto desde 0 sin mirar un cuderno, notas, libro, etc...\n","sería imposible para, aún no tengo esa soltura para hacerlo de cabeza.\n","\n","## 7. Estado final del Día 3\n","\n","✅ Notebook completo (11 secciones, ~54 celdas).\n","✅ ML Canvas documentado.\n","✅ EDA con hallazgo de filas sintéticas.\n","✅ Feature Engineering: 200 → 400 features.\n","✅ Baseline logístico: ROC-AUC 0.8615.\n","✅ LightGBM: ROC-AUC 0.8887.\n","✅ Análisis de umbral óptimo.\n","✅ Submission en Kaggle: Public Score 0.88874.\n","✅ Proyecto migrado a Google Colab sin incidencias.\n","\n"],"metadata":{"id":"wlZVCeP4xRfj"}}]}