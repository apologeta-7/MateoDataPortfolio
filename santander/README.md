{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpXIqodn/JCa1G039E6ATR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Santander Customer Transaction Prediction\n","### Kaggle Competition · Binary Classification · LightGBM\n","\n","![Python](https://img.shields.io/badge/Python-3.10-blue)\n","![LightGBM](https://img.shields.io/badge/LightGBM-gradient%20boosting-green)\n","![Kaggle](https://img.shields.io/badge/Kaggle-ROC--AUC%200.8887-orange)\n","\n","---\n","\n","## Descripción del proyecto\n","\n","Proyecto de Machine Learning supervisado basado en la competición oficial\n","de Kaggle del Banco Santander (2019). El objetivo es predecir si un cliente\n","realizará una transacción específica en el futuro, a partir de 200 variables\n","numéricas completamente anonimizadas.\n","\n","Este proyecto forma parte de mi portfolio como estudiante de Data Science.\n","Todo el código ha sido escrito y documentado línea a línea, con el objetivo\n","de entender cada decisión técnica y poder defenderla.\n","\n","---\n","\n","## Resultado\n","\n","| Métrica | Valor |\n","|---|---|\n","| ROC-AUC validación | 0.8887 |\n","| ROC-AUC público Kaggle | 0.8887 |\n","| ROC-AUC privado Kaggle | 0.8867 |\n","\n","---\n","\n","## Estructura del proyecto\n","```\n","santander/\n","├── diario/\n","│   ├── dia_01.md   # Configuración del entorno y estructura del proyecto\n","│   ├── dia_02.md   # Revisión de EDAs externos y toma de decisiones\n","│   └── dia_03.md   # Desarrollo completo del notebook y submission\n","├── notebook/\n","│   └── santander_lgbm.ipynb\n","├── submission/\n","│   └── submission_lgbm.csv\n","└── README.md\n","```\n","\n","---\n","\n","## Decisiones técnicas clave\n","\n","**1. Detección de filas sintéticas en test**\n","El análisis exploratorio reveló que el test contiene ~50% de filas\n","duplicadas artificialmente. Identificarlas fue imprescindible para\n","construir features de calidad en el paso siguiente.\n","\n","**2. Frequency Encoding**\n","Para cada variable, se calculó cuántas veces aparece cada valor\n","en el test real. Esta técnica amplió el dataset de 200 a 400 features\n","y fue la decisión con mayor impacto en el score final.\n","\n","**3. LightGBM con scale_pos_weight**\n","Modelo basado en árboles que captura interacciones no lineales.\n","El desbalance de clases (~9:1) se gestionó con scale_pos_weight\n","calculado dinámicamente desde los datos de entrenamiento.\n","\n","---\n","\n","## Stack tecnológico\n","\n","- Python · Pandas · NumPy\n","- Scikit-learn · LightGBM\n","- Matplotlib · Seaborn\n","- Google Colab · VS Code · GitHub\n","\n","---\n","\n","## Diario del proyecto\n","\n","Este proyecto incluye un diario de desarrollo en la carpeta `/diario`.\n","Cada entrada documenta las decisiones tomadas, los problemas encontrados\n","y los aprendizajes del día. Es una forma honesta de mostrar el proceso\n","real detrás del resultado final.\n","\n","---\n","\n","## Siguientes pasos\n","\n","- Tuning de hiperparámetros con Optuna\n","- Ensemble con XGBoost y CatBoost\n","- Cross-validation estratificado (k-fold)\n","- Análisis SHAP para interpretabilidad individual\n","\n","---\n","\n","*Proyecto desarrollado como parte de mi formación en Data Science.\n","Score verificable en mi perfil público de Kaggle.*"],"metadata":{"id":"uoq3bouH0-lz"}}]}