# ğŸ  House Prices â€“ Advanced Regression | Mateo Pascual

A clean, modular, and fully reproducible solution to the **Kaggle â€œHouse Prices: Advanced Regression Techniquesâ€** challenge.  
This repository is structured as a professional portfolio project and demonstrates **EDA, data cleaning, robust preprocessing, cross-validated modeling, and final submission**.

---

## ğŸ§­ Project Overview

**Goal:** predict house sale prices from tabular data (numerical + categorical).  
**Approach:** end-to-end pipeline in scikit-learn + LightGBM, with explicit handling of missing data, outliers, and skewed target distributions.

**Highlights**
- Context-aware imputation (`None`/median/mode) with zero leakage (inside Pipeline).
- Percentile-based winsorization (P1â€“P99) for selected numeric features.
- Target log-transform (`log1p`) to stabilize variance and improve RMSE.
- Reproducible **5-Fold CV** with Ridge vs. LightGBM.
- Final inverse transform (`expm1`) and `submission.csv`.

---

## ğŸ“ Repository Structure

kaggle-house-prices-mateo+++
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ data/
â”‚ â””â”€ README_DATA.md
â”œâ”€ notebooks/
â”‚ â”œâ”€ 01_EDA_and_Cleaning.ipynb
â”‚ â”œâ”€ 02_Modeling_and_Validation.ipynb
â”‚ â””â”€ 03_Prediction_Submission.ipynb
â”œâ”€ src/
â”‚ â”œâ”€ prep.py
â”‚ â””â”€ model.py
â”œâ”€ results/
â”‚ â”œâ”€ cv_scores.json
â”‚ â””â”€ figs/
â””â”€ submission/
â””â”€ submission.csv

markdown
Copiar cÃ³digo

- **notebooks/**: polished notebooks for EDA, modeling, and inference (in order).
- **src/**: reusable code for preprocessing and modeling.
- **results/**: cross-validation metrics and figures.
- **submission/**: final Kaggle submission file.
- **data/**: instructions to obtain the dataset (no raw CSVs in Git).

---

## âš™ï¸ Methodology

1. **EDA & Cleaning**
   - Drop high-missing columns: `PoolQC`, `Alley`, `Fence`, `MiscFeature`.
   - Inspect distributions and target skew.

2. **Imputation (context-aware)**
   - *Absence semantics*: `Garage*`, `Bsmt*`, `FireplaceQu`, `MasVnrType` â†’ `"None"`.
   - *Numeric partners*: `GarageArea`, `TotalBsmtSF`, `MasVnrArea`, etc. â†’ `0`.
   - `LotFrontage` â†’ **median per `Neighborhood`**.
   - `Electrical` â†’ global mode.

3. **Outliers**
   - Winsorization (clip to **P1â€“P99**) on `GrLivArea`, `LotArea`, `TotalBsmtSF`.

4. **Target Transform**
   - `SalePrice_log = log1p(SalePrice)`; prediction inverse via `expm1()`.

5. **Pipeline & Models**
   - `ColumnTransformer`: numeric (median + StandardScaler), categorical (mode + OneHotEncoder).
   - Models: **Ridge** and **LightGBM**.
   - Validation: **KFold(5)**, metric **RMSE (log scale)**.

6. **Tuning (light)**
   - Grid search over `num_leaves`, `learning_rate`, `n_estimators`.

---

## ğŸ“Š Results

| Model | RMSE (log) | Notes |
|------|-------------|-------|
| Ridge | ~0.1400 | Strong linear baseline |
| LightGBM (baseline) | ~0.1368 | Captures non-linear interactions |
| LightGBM (tuned) | ~0.1371 | Confirms stable baseline |

> **Interpretation:** RMSE_log â‰ˆ 0.14 â†’ ~15% relative error on price (â‰ˆ Â±$27k around a $180k home).

---

## ğŸš€ Reproducibility

```bash
# 1) Install dependencies
pip install -r requirements.txt

# 2) Place Kaggle CSVs under /data (see data/README_DATA.md)

# 3) Run notebooks in order:
# notebooks/01_EDA_and_Cleaning.ipynb
# notebooks/02_Modeling_and_Validation.ipynb
# notebooks/03_Prediction_Submission.ipynb
ğŸ§° Tech Stack
Python 3.11 Â· Pandas Â· NumPy Â· Matplotlib Â· Seaborn Â· Scikit-Learn Â· LightGBM

ğŸ“Œ Notes
No raw data is committed to the repo.

All preprocessing occurs inside the scikit-learn Pipeline to prevent leakage.

The same clipping limits learned on train are applied to test for consistency.

ğŸ‘¤ Author
Mateo Pascual â€” Data Analyst / ML Enthusiast
ğŸ“ Spain Â· Open to US/Global opportunities
